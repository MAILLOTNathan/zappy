#!/usr/bin/python3
import sys
import debug_lib
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import time
import pickle  # For saving and loading replay memory

def help_message():
    print("USAGE: ./zappy_ai -p port -n name -h machine")
    exit(0)

def check_args(TuringAI):
    for i in range(1, len(sys.argv)):
        if sys.argv[i] == "-p":
            TuringAI.port = int(sys.argv[i + 1])
        elif sys.argv[i] == "-n":
            TuringAI.team_name = sys.argv[i + 1]
            if TuringAI.team_name == "GRAPHIC":
                print("Error: Team name cannot be GRAPHIC.")
                exit(84)
        elif sys.argv[i] == "-h":
            TuringAI.host = sys.argv[i + 1]
    if (len(sys.argv) < 2) or sys.argv[1] == "-help":
        help_message()
    if sys.argv[1] == "--debug":
        TuringAI.debug = True
        return
    if TuringAI.port is None or TuringAI.team_name == "":
        help_message()

def parse_look(response):
    response = response.strip('[]').replace(',,', ',')
    tiles = [tile.strip() for tile in response.split(',')]
    food_counts = [tile.count('food') for tile in tiles]
    max_index = food_counts.index(max(food_counts))
    return max_index

def find_path(direction, conn, object):
    if direction == 0:
        conn.send_request("Take " + object)
    elif direction == 1:
        conn.send_request("Forward")
        conn.send_request("Left")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 2:
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 3:
        conn.send_request("Forward")
        conn.send_request("Right")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 4:
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Left")
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 5:
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Left")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 6:
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 7:
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Right")
        conn.send_request("Forward")
        conn.send_request("Take " + object)
    elif direction == 8:
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Right")
        conn.send_request("Forward")
        conn.send_request("Forward")
        conn.send_request("Take " + object)

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(11, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class ReplayMemory:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = deque(maxlen=capacity)

    def push(self, transition):
        self.memory.append(transition)

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class TuringAI:
    debug = False
    port = None
    team_name = ""
    host = "localhost"
    command = ["Forward", "Right", "Left", "Look", "Inventory", "Broadcast", "Connect_nbr", "Fork", "Eject", "Take", "Set", "Incantation"]

    level_requirements = {
        1: {"players": 1, "linemate": 1, "deraumere": 0, "sibur": 0, "mendiane": 0, "phiras": 0, "thystame": 0},
        2: {"players": 2, "linemate": 1, "deraumere": 1, "sibur": 1, "mendiane": 0, "phiras": 0, "thystame": 0},
        3: {"players": 2, "linemate": 2, "deraumere": 0, "sibur": 1, "mendiane": 0, "phiras": 2, "thystame": 0},
        4: {"players": 4, "linemate": 1, "deraumere": 1, "sibur": 2, "mendiane": 0, "phiras": 1, "thystame": 0},
        5: {"players": 4, "linemate": 1, "deraumere": 2, "sibur": 1, "mendiane": 3, "phiras": 0, "thystame": 0},
        6: {"players": 6, "linemate": 1, "deraumere": 2, "sibur": 3, "mendiane": 0, "phiras": 1, "thystame": 0},
        7: {"players": 6, "linemate": 2, "deraumere": 2, "sibur": 2, "mendiane": 2, "phiras": 2, "thystame": 1},
    }

    def __init__(self):
        self.debug = False
        self.port = None
        self.team_name = ""
        self.host = "localhost"
        self.command = ["Forward", "Right", "Left", "Look", "Inventory", "Broadcast", "Connect_nbr", "Fork", "Eject", "Take", "Set", "Incantation"]
        self.inventory = {"food": 0, "linemate": 0, "deraumere": 0, "sibur": 0, "mendiane": 0, "phiras": 0, "thystame": 0}
        self.level = 1

        self.state_size = 16
        self.action_size = 12
        self.memory = ReplayMemory(10000)
        self.model = DQN(self.state_size, self.action_size)
        self.target_model = DQN(self.state_size, self.action_size)
        self.optimizer = optim.Adam(self.model.parameters())
        self.criterion = nn.MSELoss()
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.995

        # Load model weights if they exist
        try:
            self.model.load_state_dict(torch.load("model_weights.pth"))
            self.target_model.load_state_dict(torch.load("model_weights.pth"))
            self.model.eval()
            self.target_model.eval()
            print("Loaded saved model weights.")
        except FileNotFoundError:
            print("No saved model weights found. Starting from scratch.")

        # Load replay memory if it exists
        try:
            with open("replay_memory.pkl", "rb") as f:
                self.memory = pickle.load(f)
            print("Loaded saved replay memory.")
        except FileNotFoundError:
            print("No saved replay memory found. Starting from scratch.")

    def select_action(self, state):
        if random.random() < self.epsilon:
            return random.randrange(self.action_size)
        with torch.no_grad():
            return self.model(torch.FloatTensor(state)).argmax().item()

    def train_step(self, batch_size):
        if len(self.memory) < batch_size:
            return
        transitions = self.memory.sample(batch_size)
        batch = list(zip(*transitions))

        states = torch.FloatTensor(batch[0])
        actions = torch.LongTensor(batch[1])
        rewards = torch.FloatTensor(batch[2])
        next_states = torch.FloatTensor(batch[3])
        dones = torch.FloatTensor(batch[4])

        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        next_q_values = self.target_model(next_states).max(1)[0]
        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

        loss = self.criterion(q_values, target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def get_state(self, response):
        tiles = response.strip('[]').split(',')
        state = [tile.count('food') for tile in tiles]
        state.extend(self.inventory.values())
        return np.array(state, dtype=np.float32)

    def check_level_up(self):
        requirements = self.level_requirements[self.level]
        for item in requirements:
            if item == "players":
                continue
            if self.inventory.get(item, 0) < requirements[item]:
                return False
        return True

    def save_progress(self):
        # Save model weights
        torch.save(self.model.state_dict(), "model_weights.pth")
        # Save replay memory
        with open("replay_memory.pkl", "wb") as f:
            pickle.dump(self.memory, f)
        print("Progress saved.")

def parse_inventory(response):
    items = response.strip('{}').split(',')
    inventory = {}
    for item in items:
        k, v = item.split()
        inventory[k] = int(v)
    return inventory

def basic_ia(conn, ai):
    start_time = time.time()
    reward = 0
    state = ai.get_state(conn.send_request("Look").decode())
    while ai.level < 3:
        action = ai.select_action(state)
        if action == 0:
            res = conn.send_request("Forward")
            if res == "dead":
                break
        elif action == 1:
            res = conn.send_request("Right")
            if res == "dead":
                break
        elif action == 2:
            res = conn.send_request("Left")
            if res == "dead":
                break
        elif 3 <= action < 10:
            item = list(ai.inventory.keys())[action - 3]
            res = conn.send_request(f"Take {item}")
            if res == "dead":
                break
        elif action == 11:
            if ai.check_level_up():
                res = conn.send_request("Incantation")
                if res == "dead":
                    break
                ai.level += 1  # Increment level after successful incantation
                reward = 10  # Reward for leveling up
            else:
                reward = 0
        else:
            reward = 1 / (time.time() - start_time)  # Default survival reward

        try :
            next_state = ai.get_state(conn.send_request("Look").decode())
        except:
            break
        done = False

        ai.memory.push((state, action, reward, next_state, done))
        state = next_state

        ai.train_step(32)

        if done:
            state = ai.get_state(conn.send_request("Look").decode())

        ai.update_target_model()

    print(ai.level)
    ai.save_progress()

def main():
    ai = TuringAI()
    check_args(ai)
    conn = debug_lib.ServerConnection(ai)
    if ai.debug:
        conn.connect_to_server_debug()
    conn.connect_to_server(ai.team_name)
    basic_ia(conn, ai)

if __name__ == "__main__":
    main()
